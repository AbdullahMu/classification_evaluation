{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Evaluating Classifiers: The Confusion Matrix and AUC-ROC\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "\n",
    "### Core\n",
    "- Understand what true positives, false positives, true negatives, and false negatives are\n",
    "- Understand what a confusion matrix is and how to apply and use it\n",
    "- Calculate the most common classification evaluation metrics\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1\n",
    "- Produce a classification report\n",
    "- Plot a receiver operating characteristic (ROC) curve\n",
    "\n",
    "### Target\n",
    "\n",
    "- Know when you would choose to change the threshold for prediction for a classification model\n",
    "- Manually change the threshold for prediction and observe the results\n",
    "- Calculate the ROC-AUC score\n",
    "\n",
    "\n",
    "### Stretch\n",
    "- Describe what ROC curves are and why they're useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction: evaluating classifiers](#intro)\n",
    "- [The baseline accuracy](#baseline)\n",
    "- [Build a KNN model to predict spam](#knn)\n",
    "- [Predicting labels vs. predicting probabilities](#labels-vs-probs)\n",
    "- [The confusion matrix](#confusion-matrix)\n",
    "- [Review: Type I error and p-values](#type1-pvalues)\n",
    "- [Type II error and power](#type2-power)\n",
    "- [Fundamental classifier metrics](#metrics)\n",
    "    - [Accuracy](#accuracy)\n",
    "    - [Sensitivity / Recall / True Positive Rate (TPR)](#sensitivity)\n",
    "    - [False Positive Rate (FPR)](#fpr)\n",
    "    - [Specificity / True Negative Rate (TNR)](#specificity)\n",
    "    - [Precision / Positive Predictive Value](#precision)\n",
    "- [The F1-score and sklearn's `classification_report`](#f1-score)\n",
    "- [Changing the threshold for prediction](#threshold)\n",
    "    - [Load the UCI breast cancer data](#uci)\n",
    "    - [Evaluate prediction on a test set](#testing)\n",
    "- [The ROC curve](#roc-curve)\n",
    "- [Reference table of common classification metric terms and definitions](#table)\n",
    "- [Additional resources](#resources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction: evaluating classifiers\n",
    "\n",
    "---\n",
    "\n",
    "The evaluation of classifier models is not as straightforward as for regression models. In this lesson we will cover the fundamentals of evaluating classifiers.\n",
    "\n",
    "To ground the theory in a real world example, we will use a spam dataset to build and evaluate classifiers on. The spam dataset is 1001 columns wide, containing an `is_spam` binary marker for whether a given email was spam or not, and then 1000 columns, each corresponding to a word that could have appeared in the email (also marked with 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the spam data below and print out the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv('spam_words_wide.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam[spam.is_spam==1].sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spam.shape, spam.is_spam.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spam), len(spam[spam.is_spam==1]),len(spam[spam.is_spam==0]), len(spam[spam.is_spam==1])/float(len(spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "\n",
    "## The baseline accuracy\n",
    "\n",
    "---\n",
    "\n",
    "The importance of calculating your baseline accuracy when building classifiers cannot be overstated. It is critical to know the baseline when you are evaluating a classifier using accuracy.\n",
    "\n",
    "> **Baseline Accuracy**: The accuracy that can be achieved by a model by simply guessing the majority class for every observation.\n",
    "\n",
    "As human beings we are inclined to think that \"50% accuracy\" is equivalent to guessing by chance. In fact, a 50% accuracy only equates to guessing by chance in a very specific context: when we have equal proportion of positive and negative (1 and 0) target class labels in our dataset, or in the multi-class case when the majority class makes up 50% of the labels.\n",
    "\n",
    "> **`baseline_accuracy = majority_class_N / total_N`**\n",
    "\n",
    "In a binary class problem the reality is that your dataset is more likely to be unbalanced, and the more unbalanced it is the higher the baseline accuracy becomes. This is important to remember because if 99% of your observations are of one class, predicting 99% of them correctly with a model is performing at chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the baseline accuracy for the spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_acc = 1. - spam.is_spam.mean()\n",
    "baseline_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a KNN model to predict spam\n",
    "\n",
    "---\n",
    "\n",
    "We will use just the first 250 columns for the sake of speed in fitting and prediction. Even with a fourth of the predictors the cross-validation can be noticeably slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validate the accuracy of the model\n",
    "\n",
    "Use 10 folds. How does the mean performance across folds compare to the baseline accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification models use stratified k-fold by default. This does maintain class balance between the different folds, however also preserves order of the observations. The explicit formulation of the last step can be achieved like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling of the dataset can be useful if the data has been arranged in a specific order. Here it hardly makes any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='labels-vs-probs'></a>\n",
    "\n",
    "## Predicting labels vs. predicting probabilities\n",
    "\n",
    "---\n",
    "\n",
    "Sklearn classification models come with two distinct prediction functions:\n",
    "1. `.predict()`: predicts the labels (classes) of observations \n",
    "2. `.predict_proba()` predicts the *probability of membership to each class*.\n",
    "\n",
    "The `.predict()` function will return the predicted labels for a design matrix as a vector of integer labels. \n",
    "\n",
    "In contrast, the `.predict_proba()` function will return the probabilities as a matrix, where the columns are ordered in increasing order of the class labels (e.g. the first column is probabilities for class 0, the second column is probabilities for class 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the KNN model and print out the predicted labels and predicted probabilities for a few points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Class predictions:\", knn_predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True class labels:\", y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted probabilities:\\n\", knn.predict_proba(X.iloc[0:10, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of classification errors:\", np.abs(knn_predictions-y).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='confusion-matrix'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The confusion matrix is a table representing the performance of your model to classify labels correctly.\n",
    "\n",
    "**A confusion matrix for a binary classification task:**\n",
    "\n",
    "|   |Predicted Positive | Predicted Negative |   \n",
    "|---|---|---|\n",
    "|**Actual Positive** | True Positive (TP)  | False Negative (FN)  |  \n",
    "|**Actual Negative**  | False Positive (FP)  | True Negative (TN)  | \n",
    "\n",
    "In a binary classifier, the \"true\" class is typically labeled with 1 and the \"false\" class is labeled with 0. \n",
    "\n",
    "> **True Positive**: A positive class observation (1) is correctly classified as positive by the model.\n",
    "\n",
    "> **False Positive**: A negative class observation (0) is incorrectly classified as positive.\n",
    "\n",
    "> **True Negative**: A negative class observation is correctly classified as negative.\n",
    "\n",
    "> **False Negative**: A positive class observation is incorrectly classified as negative.\n",
    "\n",
    "Columns of the confusion matrix sum to the predictions by class. Rows of the matrix sum to the actual values within each class. You may encounter confusion matrices where the actual is in columns and the predicted is in the rows: the meaning is the same but the table will be reoriented.\n",
    "\n",
    "> **Note:** Remembering what the cells in the confusion matrix represent can be a little tricky. The first word (True or False) indicates whether or not the model was correct. The second word (Positive or Negative) indicates the *model's guess* (not the actual label!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the confusion matrix metrics for your model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = knn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.sum((y == 1) & (predicted == 1))\n",
    "fp = np.sum((y == 0) & (predicted == 1))\n",
    "tn = np.sum((y == 0) & (predicted == 0))\n",
    "fn = np.sum((y == 1) & (predicted == 0))\n",
    "print(\"tp:\", tp)\n",
    "print(\"fp:\", fp)\n",
    "print(\"tn:\", tn) \n",
    "print(\"fn:\", fn)\n",
    "print(\"Number of classification errors:\", fp+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify this is the same as the numbers you get from sklearn's `metrics.confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='type1-pvalues'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Review: Type I error and p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "In the context of hypothesis testing false positives and false negatives are referred to as Type I and Type II error, respectively. \n",
    "\n",
    "Type I error is the incorrect rejection of the null hypothesis when in fact the null hypothesis is true. This is equivalent to the false positive rate in classification: the rate of a model labeling an observation as \"true\" when in fact it is \"false\". \n",
    "\n",
    "Type I error directly corresponds to p-values: **the p-value is the probability of incorrectly rejecting the null hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Type II error and \"power\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "Type II error, on the other hand, directly corresponds to false negatives. A Type II error in the context of hypothesis testing would be to accept the null hypothesis when in fact the alternative hypothesis is true. \n",
    "\n",
    "Whereas Type I error corresponds to the concept of _statistical significance_, Type II error corresponds to the concept of _statistical power._ The power of a test is:\n",
    "\n",
    "$$ \\text{power} = 1 - P(\\text{Type II error}) $$\n",
    "\n",
    "More intuitively, **power measures our ability to detect an effect that is present.**\n",
    "\n",
    "We can visualize the ideas of significance, power, and error types in a matrix the same as our confusion matrix from above:\n",
    "\n",
    "|   |Accept $H_0$ | Reject $H_0$ |   \n",
    "|---|---|---|\n",
    "|**$H_0$ is True** | P(correct) <br> _(1 - alpha)_  | P(type I error) <br> _(alpha, significance)_  |  \n",
    "|**$H_0$ is False**  | P(type II error) <br> _(beta)_  | P(correct) <br> _(1 - beta, power)_ | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fundamental classifier metrics\n",
    "\n",
    "---\n",
    "\n",
    "All metrics we use to evaluate classifiers are tied to the content of the confusion matrix. Remember that for the binary classification case we have four cells in the confusion matrix:\n",
    "\n",
    "- **`tp`**: true positives (classifier correct; classifier guessed 1)\n",
    "- **`fp`**: false positives (classifier incorrect; classifier guessed 1)\n",
    "- **`tn`**: true negatives (classifier correct; classifier guessed 0)\n",
    "- **`fn`**: false negatives (classifier incorrect; classifier guessed 0)\n",
    "\n",
    "Below are the fundamental metrics that data scientists use to evaluate the performance of their classifier model.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='accuracy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "With the total population as\n",
    "\n",
    "> **`total_population = tp + fp + tn + fn`**\n",
    "\n",
    "the accuracy can be calculated as\n",
    "\n",
    "> **`accuracy = (tp + tn) / total_population`**\n",
    "\n",
    "which is just the proportion of correct guesses, regardless of class. The `.score()` function attached to sklearn classification model objects defaults to returning the accuracy of the model's predictions given an `X` and `y`.\n",
    "\n",
    "The difference between one and the accuracy is known as the **misclassification rate**, which is calculated as:\n",
    "\n",
    "> **`misclassification_rate = (fp + fn) / total_population`**\n",
    "\n",
    "**Calculate the accuracy using the confusion matrix cells.**\n",
    "- Validate that it is the same as `metrics.accuracy_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "total_population = tp + fp + tn + fn\n",
    "\n",
    "print(float(tp + tn) / total_population)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='precision'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision / Positive Predictive Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision measures out of all the times the classifier predicted a positive label, the true label was also positive.\n",
    "\n",
    "> **`precision = tp / (tp + fp)`**\n",
    "\n",
    "The idea of the classifier being _precise_ is subtly different than it being _accurate_. Precision is a measure of correctness only for its positive class predictions, whereas accuracy is a measure of correctness for all guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the precision using the confusion matrix cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Validate that this is the same as `metrics.precision_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(float(tp) / (tp + fp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='sensitivity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall/ Sensitivity / True Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **recall** measures out of all the times the true label was positive, the predicted label was also positive.\n",
    " \n",
    "\n",
    "This is alternatively known as the **sensitivity** or **true positive rate**. The three names refer to the same quantity. In most cases, we will use the name recall.\n",
    "\n",
    "This is calculated as:\n",
    "\n",
    "> **`recall = tp / (tp + fn)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the recall with the confusion matrix cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Validate that this is the same as `metrics.recall_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(float(tp) / (tp + fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='fpr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate (FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the false positive rate measures out of all the times the true label was negative, the predicted label was positive.\n",
    "  \n",
    "\n",
    "> **`fpr = fp / (tn + fp)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the FPR using the confusion matrix cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(float(fp) / (tn + fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to calculate the same\n",
    "1 - recall_score(y==0, predicted==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='specificity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity / True Negative Rate (TNR)\n",
    "\n",
    "The true negative rate measures out of all the times the true label was negative, the predicted label was also negative.\n",
    " \n",
    "It is the sister metric to recall, which measures the same but for positives.\n",
    "\n",
    "> **`specificity = tn / (tn + fp)`**\n",
    "\n",
    "**Calculate the specificity using the confusion matrix cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = float(tn) / (tn + fp)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to calculate the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f1-score'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-score is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of the precision and recall metrics.  \n",
    "\n",
    "$$ F_1 = 2 \\cdot \\frac{1}{\\tfrac{1}{\\mathrm{recall}} + \\tfrac{1}{\\mathrm{precision}}} = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$$\n",
    "\n",
    "Also this score gives a number between zero and one which lies between the respective values of precision and mean.\n",
    "\n",
    "Blending the two is useful.\n",
    "By combining the two we have a measure of the classifiers' ability to find the positively labeled observations as well as how permissive it is of identification errors on those labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision_1 = float(tp)/(tp+fp)\n",
    "recall_1 = float(tp)/(tp+fn)\n",
    "f1_1 = 2/(1/recall_1+1/precision_1)\n",
    "print(f1_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn's `metrics.classification_report` prints out the report of the three metrics precision, recall and $F_1$-score on both of the classes (or more if you have a multi-class problem). \"Support\" refers to the total number of observations in each class. The classification report helps diagnose the effectiveness of your classifier.\n",
    "\n",
    "- The row labeled as 1 uses our definitions above.\n",
    "- The row labeled as 0 uses the definitions above if label 0 was considered as the positive class, hence it contains the true negative rate in the recall column.\n",
    "- The row labeled average reports the weighted averages across both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_1 = precision_score(y==1, predicted==1) \n",
    "precision_0 = precision_score(y==0, predicted==0)\n",
    "\n",
    "recall_1 = recall_score(y==1, predicted==1) \n",
    "recall_0 = recall_score(y==0, predicted==0)\n",
    "\n",
    "f1_1 = f1_score(y==1, predicted==1)\n",
    "f1_0 = f1_score(y==0, predicted==0)\n",
    "\n",
    "average_precision = (1-baseline_acc)*precision_1+baseline_acc*precision_0\n",
    "average_recall = (1-baseline_acc)*recall_1+baseline_acc*recall_0\n",
    "average_f1 = (1-baseline_acc)*f1_1+baseline_acc*f1_0\n",
    "\n",
    "print('\\t Precision\\t   ','Recall\\t      ','F_1')\n",
    "print('0\\t',precision_0, recall_0, f1_0)\n",
    "print('1\\t',precision_1, recall_1, f1_1)\n",
    "print('Avg\\t',average_precision, average_recall, average_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='threshold'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the threshold for prediction\n",
    "\n",
    "---\n",
    "\n",
    "The prediction of the classifier defaults to guessing the class that has the highest predicted probability. This necessarily leads to the highest possible accuracy (**only a guarantee for the training data!**). \n",
    "\n",
    "However, it could be the case that maximizing the accuracy is not, in fact, our ultimate goal. Consider the following scenario:\n",
    "\n",
    "> **Cancer detection:** You have developed a classifier to detect, based on some medical measurements, whether or not a person has a cancerous tumor or not. Your classifier gets a 96% accuracy compared to a 60% baseline accuracy.\n",
    "\n",
    "Your classifier is performing well, but what might be wrong with just maximizing the accuracy in this case? Think back to the confusion matrix and your goal (to treat cancer patients before it is too late)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the UCI breast cancer data\n",
    "\n",
    "Below we will load the medical data on breast cancer detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id',\n",
    "                'clump_thickness',\n",
    "                'cell_size_uniformity',\n",
    "                'cell_shape_uniformity',\n",
    "                'marginal_adhesion',\n",
    "                'single_epithelial_size',\n",
    "                'bare_nuclei',\n",
    "                'bland_chromatin',\n",
    "                'normal_nucleoli',\n",
    "                'mitoses',\n",
    "                'class']\n",
    "\n",
    "bcw = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', \n",
    "                  names=column_names)\n",
    "\n",
    "bcw['bare_nuclei'] = bcw.bare_nuclei.map(lambda x: int(x) if not x == '?' else np.nan)\n",
    "bcw.dropna(inplace=True)\n",
    "\n",
    "y = bcw['class'].map(lambda x: 1 if x == 4 else 0)\n",
    "X = bcw.iloc[:, 1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the accuracy on the test set and compare to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the predicted labels and predicted probabilities on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the confusion matrix for your classfier's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say again that we are predicting cancer based on some kind of detection measure, as before.\n",
    "conmat = np.array(confusion_matrix(y_test, y_pred, labels=[1,0]))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['is_cancer', 'is_healthy'],\n",
    "                         columns=['predicted_cancer','predicted_healthy'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='change-threshold'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower the threshold for predicting cancer\n",
    "\n",
    "Right now the classifier is choosing to label cancer vs. healthy based on the 0.5 predicted probability threshold. \n",
    "\n",
    "Say our goal was to have 0 false negatives: in other words, in no case do we want to predict the person is healthy when in fact they have cancer!\n",
    "\n",
    "1. Create a dataframe of the predicted probabilities (class 0 and class 1 probabilities).\n",
    "2. Create a new column with predicted labels where the threshold for labeling cancer/1 is 10% rather than 50%\n",
    "    - In other words, the predicted probability of class 1 only needs to be greater than 0.10 for the label to be 1.\n",
    "3. Recreate the confusion matrix with the predictions using the new threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted probability vector and explicitly name the columns:\n",
    "Y_pp = pd.DataFrame(knn.predict_proba(X_test), columns=['class_0_pp','class_1_pp'])\n",
    "Y_pp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In order to do this, we can lower the threshold for predicting class 1.\n",
    " This will reduce our false negative rate to 0, but at the expense of a higher false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conmat = np.array(confusion_matrix(y_test, Y_pp.pred_class_thresh10.values, labels=[1,0]))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['is_cancer', 'is_healthy'],\n",
    "                         columns=['predicted_cancer','predicted_healthy'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='roc-curve'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Receiver operating characteristic (ROC) curve\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is a popular visual of the performance of a classifier. It has a few attractive properties:\n",
    "- It plots the **true positive rate (TPR)** versus the **false positive rate (FPR)** as the threshold for predicting 1 changes.\n",
    "- We measure the area under the curve.\n",
    "- If the TPR is always 1, the area under the curve is 1 (it cannot be larger). This is equivalent to perfect prediction.\n",
    "- When the area under the curve is 0.50, this is equivalent to the baseline (chance) prediction.\n",
    "- If the area is smaller than 0.5, it would be better to swap the predicted labels. \n",
    "\n",
    "Even though the area under the ROC curve is directly related to the accuracy, the AUC-ROC is preferred over the accuracy because it is automatically adjusted to the baseline and gives a robust picture of how the classifier performs at different threshold choices. \n",
    "\n",
    "**Note:**\n",
    "- As the class assignment threshold increases for the positive class (has cancer), the false positive rate and true positive rate necessarily increase.\n",
    "- A classifier performing at chance corresponds to the diagonal dotted line: an equal chance of false positives and true positives.\n",
    "- The greater the area under the curve, the higher the ratio of true positives to false positives as the threshold becomes more lenient.\n",
    "- The greater the area under the curve, the higher the quality of the classification model. \n",
    "- Most classification problems will never get close to a full 1.0 area under the curve: the Wisconsin breast cancer data is unique in that the signal is extremely strong. If you see this, it often indicates that something is wrong with your procedure (or, if you are predicting on the training set, that your model is overfitting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is code to plot the ROC curve for our cancer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For class 1, find the area under the curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, Y_pp.class_1_pp)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc, linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for cancer detection', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='table'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('fpr\\t', 'tpr\\t', 'threshold')\n",
    "print(np.array(list(zip(fpr,tpr,threshold))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference table of common classification metric terms and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "|  TERM | DESCRIPTION  |\n",
    "|---|---|\n",
    "|**TRUE POSITIVES** | The number of \"true\" classes correctly predicted to be true by the model. <br><br> `TP = Sum of observations predicted to be 1 that are actually 1`<br><br>The true class in a binary classifier is labeled with 1.|\n",
    "|**TRUE NEGATIVES** | The number of \"false\" classes correctly predicted to be false by the model. <br><br> `TN = Sum of observations predicted to be 0 that are actually 0`<br><br>The false class in a binary classifier is labeled with 0.|\n",
    "|**FALSE POSITIVES** | The number of \"false\" classes incorrectly predicted to be true by the model. This is the measure of **Type I error**.<br><br> `FP = Sum of observations predicted to be 1 that are actually 0`<br><br>Remember that the \"true\" and \"false\" refer to the veracity of your guess, and the \"positive\" and \"negative\" component refer to the guessed label.|\n",
    "|**FALSE NEGATIVES** | The number of \"true\" classes incorrectly predicted to be false by the model. This is the measure of **Type II error.**<br><br> `FN = Sum of observations predicted to be 0 that are actually 1`<br><br>|\n",
    "|**TOTAL POPULATION** | In the context of the confusion matrix, the sum of the cells. <br><br> `total population = TP + TN + FP + FN`<br><br>|\n",
    "|**SUPPORT** | The marginal sum of rows in the confusion matrix, or in other words the total number of observations belonging to a class regardless of prediction. <br><br>|\n",
    "|**ACCURACY** | The number of correct predictions by the model out of the total number of observations. <br><br> `accuracy = (TP + TN) / total_population`<br><br>|\n",
    "|**PRECISION** | The ability of the classifier to avoid labeling a class as a member of another class. <br><br> `Precision = TP / (TP + FP)`<br><br>_A precision score of 1 indicates that the classifier never mistakenly classified the current class as another class.  precision score of 0 would mean that the classifier misclassified every instance of the current class_ |\n",
    "|**RECALL/SENSITIVITY/TRUE POSITIVE RATE**    | The ability of the classifier to correctly identify the current class. <br><br>`Recall = TP / (TP + FN)`<br><br>A recall of 1 indicates that the classifier correctly predicted all observations of the class.  0 means the classifier predicted all observations of the current class incorrectly.|\n",
    "|**SPECIFICITY** | Percent of times the classifier predicted 0 out of all the times the class was 0.<br><br> `specificity = TN / (TN + FP)`<br><br>|\n",
    "|**FALSE POSITIVE RATE** | Percent of times model predicts 1 when the class is 0.<br><br> `fpr = FP / (TN + FP)`<br><br>|\n",
    "|**F1-SCORE** | The harmonic mean of precision and recall. The harmonic mean is used here rather than the more conventional arithmetic mean because the harmonic mean is more appropriate for averaging rates. <br><br>`F1-Score = 2 * (Precision * Recall) / (Precision + Recall)` <br><br>_The f1-score's best value is 1 and the worst value is 0, like the precision and recall scores. It is a useful metric for taking into account both measures at once._ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- An introduction to [Confusion Matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n",
    "- A deeper [Introduction to ROC](http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf)\n",
    "- Receiver Operation Characteristic curves, [university resource](http://ebp.uga.edu/courses/Chapter%204%20-%20Diagnosis%20I/8%20-%20ROC%20curves.html)\n",
    "- Interactive [playing with ROC curves](http://www.navan.name/roc/)\n",
    "- Data School's video and transcript on [ROC/AUC](http://www.dataschool.io/roc-curves-and-auc-explained/)\n",
    "- Watch Rahul Patwari's [video](https://www.youtube.com/watch?v=21Igj5Pr6u4) on ROC curves"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
